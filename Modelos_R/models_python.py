# -*- coding: utf-8 -*-
"""models_python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BfMX0MzENL8swrw0XnRMuTJwgvwL-CqZ
"""

import pandas as pd
import numpy as np
import keras

import xgboost

from sklearn.model_selection import train_test_split

from keras.optimizers import Adam

from keras.models import Model

from keras.layers import Activation, Dense, Dropout



from keras.models import Sequential

from keras.wrappers.scikit_learn import KerasRegressor

from sklearn.model_selection import cross_val_score

from sklearn.model_selection import KFold

from sklearn.preprocessing import MinMaxScaler

from sklearn.pipeline import Pipeline

data = pd.read_csv('Modelar_UH2019.txt', sep = '|', encoding = 'utf-8')

data.drop(['HY_id', 'HY_cod_postal', 'HY_descripcion', 'HY_distribucion'], axis = 1, inplace = True)


df = pd.concat([data,pd.get_dummies(data['HY_provincia'], prefix='HY_provincia', drop_first = True)],axis=1)

# now drop the original 'country' column (you don't need it anymore)
df.drop(['HY_provincia'],axis=1, inplace=True)

df = pd.concat([df, pd.get_dummies(df['HY_tipo'], prefix = 'HY_tipo', drop_first = True)], axis = 1)

df.drop(['HY_tipo'], axis = 1, inplace = True)

for columna in df.columns:
    
    if (np.sum(df[columna].isnull()) / df.shape[0]) > 0.3 :
        
        print('la columna ' + str(columna) + ' tiene más de 30% de NAs')
        
        df.drop(columna, axis = 1, inplace = True)
        
    elif (np.sum(df[columna].isnull()) / df.shape[0]) > 0.1:
        
        print('la columna ' + str(columna) + ' tiene más de 10% de NAs')
        
    else:
        
        next
        

df_no_na = df.dropna(axis = 0)

df_no_na.shape
#(5036, 98)

df_no_na['HY_metros_utiles'] = np.log1p(df_no_na['HY_metros_utiles'])

df_no_na['HY_metros_totales'] = np.log1p(df_no_na['HY_metros_totales'])

df_no_na['TARGET'] = np.log1p(df_no_na['TARGET'])

X = np.array(df_no_na.drop('TARGET', axis = 1))

Y = np.array(df_no_na['TARGET']).reshape((5036, 1))



def GammaTeamEstimator(dropout = 0.2):
  
  
  #Model Creation
  
  model = Sequential()
  
  #first hidden layer
  model.add(Dense(40, input_dim=97, kernel_initializer='normal', activation='relu'))
  
  model.add(Dropout(dropout))
  
  #second hidden layer
  model.add(Dense(20, kernel_initializer = 'normal', activation = 'relu'))
  
  model.add(Dropout(dropout))
  
  #Third hidden layer
  model.add(Dense(10, kernel_initializer = 'normal', activation = 'relu'))
  
  model.add(Dropout(dropout/2))
  
  #Fourth hidden layer (output neuron)
  model.add(Dense(1, kernel_initializer = 'normal'))
  
  #Compile Model. 
  model.compile(loss = 'mean_absolute_error',
               optimizer = 'adam')
  
  return model


np.random.seed(1)
estimators = []
estimators.append(('standardize', MinMaxScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=GammaTeamEstimator, epochs=60, batch_size=12, verbose=1)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10, random_state=7)
results = cross_val_score(pipeline, X, Y, cv=kfold)
print("Wider: %.2f (%.2f) Median Absolute Error" % (results.mean(), results.std()))


## XG Boost

xgb = xgboost.XGBRegressor(colsample_bytree=0.4,
                 gamma=0,                 
                 learning_rate=0.07,
                 max_depth=3,
                 min_child_weight=1.5,
                 n_estimators=10000,                                                                    
                 reg_alpha=0.75,
                 reg_lambda=0.45,
                 subsample=0.6,
                 seed=42) 

X_train, y_train, X_test, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 7)

xgb.fit(train_dataset[every_column_except_y],train_dataset['SalePrice'])

OrderedDict(sorted(model.booster().get_fscore().items(), key=lambda t: t[1], reverse=True))